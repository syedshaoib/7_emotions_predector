{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac96045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "93058fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "(\n",
      "2\n",
      ")\n",
      "/\n",
      "C\n",
      "K\n",
      "+\n",
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "path='archive (2)/CK+48'\n",
    "for i in path:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4a5daa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 981 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   validation_split=0.6,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('archive (2)/CK+48',\n",
    "                                                 target_size = (48,48),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3a07c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img=cv2.imread('archive (2)\\CK+48\\sadness\\S014_002_00000015.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c1ac79fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f49938f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_64 (Conv2D)          (None, 11, 11, 32)        4736      \n",
      "                                                                 \n",
      " max_pooling2d_64 (MaxPoolin  (None, 5, 5, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " layer_normalization_56 (Lay  (None, 5, 5, 32)         64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 5, 5, 64)          51264     \n",
      "                                                                 \n",
      " max_pooling2d_65 (MaxPoolin  (None, 2, 2, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " layer_normalization_57 (Lay  (None, 2, 2, 64)         128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 2, 2, 256)         147712    \n",
      "                                                                 \n",
      " max_pooling2d_66 (MaxPoolin  (None, 1, 1, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " layer_normalization_58 (Lay  (None, 1, 1, 256)        512       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 339,591\n",
      "Trainable params: 339,591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(48,48, 3), filters=32, kernel_size=(7, 7), strides=4, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=7, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "063b319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "812bf916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "31/31 [==============================] - 6s 166ms/step - loss: 2.1068 - accuracy: 0.2049 - val_loss: 1.9275 - val_accuracy: 0.1804\n",
      "Epoch 2/25\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 1.9097 - accuracy: 0.2375 - val_loss: 1.8424 - val_accuracy: 0.2538\n",
      "Epoch 3/25\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 1.8549 - accuracy: 0.2691 - val_loss: 1.7389 - val_accuracy: 0.3364\n",
      "Epoch 4/25\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 1.4463 - accuracy: 0.4924 - val_loss: 1.2093 - val_accuracy: 0.5678\n",
      "Epoch 5/25\n",
      "31/31 [==============================] - 4s 129ms/step - loss: 1.1137 - accuracy: 0.5810 - val_loss: 0.9197 - val_accuracy: 0.6381\n",
      "Epoch 6/25\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 0.9873 - accuracy: 0.6198 - val_loss: 0.8378 - val_accuracy: 0.6799\n",
      "Epoch 7/25\n",
      "31/31 [==============================] - 4s 129ms/step - loss: 0.8496 - accuracy: 0.6769 - val_loss: 0.6389 - val_accuracy: 0.7635\n",
      "Epoch 8/25\n",
      "31/31 [==============================] - 4s 126ms/step - loss: 0.7482 - accuracy: 0.7166 - val_loss: 0.6255 - val_accuracy: 0.7503\n",
      "Epoch 9/25\n",
      "31/31 [==============================] - 4s 128ms/step - loss: 0.7221 - accuracy: 0.7380 - val_loss: 0.5937 - val_accuracy: 0.7676\n",
      "Epoch 10/25\n",
      "31/31 [==============================] - 4s 130ms/step - loss: 0.8121 - accuracy: 0.6962 - val_loss: 0.6562 - val_accuracy: 0.7401\n",
      "Epoch 11/25\n",
      "31/31 [==============================] - 4s 139ms/step - loss: 0.5867 - accuracy: 0.7727 - val_loss: 0.5470 - val_accuracy: 0.7890\n",
      "Epoch 12/25\n",
      "31/31 [==============================] - 5s 150ms/step - loss: 0.5530 - accuracy: 0.7920 - val_loss: 0.4672 - val_accuracy: 0.8338\n",
      "Epoch 13/25\n",
      "31/31 [==============================] - 4s 137ms/step - loss: 0.4791 - accuracy: 0.8135 - val_loss: 0.5074 - val_accuracy: 0.7910\n",
      "Epoch 14/25\n",
      "31/31 [==============================] - 4s 133ms/step - loss: 0.4893 - accuracy: 0.8236 - val_loss: 0.4038 - val_accuracy: 0.8491\n",
      "Epoch 15/25\n",
      "31/31 [==============================] - 4s 130ms/step - loss: 0.5403 - accuracy: 0.8073 - val_loss: 0.5060 - val_accuracy: 0.7941\n",
      "Epoch 16/25\n",
      "31/31 [==============================] - 4s 129ms/step - loss: 0.4919 - accuracy: 0.8043 - val_loss: 0.4789 - val_accuracy: 0.8277\n",
      "Epoch 17/25\n",
      "31/31 [==============================] - 4s 132ms/step - loss: 0.4439 - accuracy: 0.8257 - val_loss: 0.4316 - val_accuracy: 0.8318\n",
      "Epoch 18/25\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 0.3775 - accuracy: 0.8552 - val_loss: 0.3880 - val_accuracy: 0.8624\n",
      "Epoch 19/25\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 0.4168 - accuracy: 0.8410 - val_loss: 0.3185 - val_accuracy: 0.8818\n",
      "Epoch 20/25\n",
      "31/31 [==============================] - 4s 130ms/step - loss: 0.4103 - accuracy: 0.8563 - val_loss: 0.3113 - val_accuracy: 0.8797\n",
      "Epoch 21/25\n",
      "31/31 [==============================] - 4s 129ms/step - loss: 0.3674 - accuracy: 0.8705 - val_loss: 0.3990 - val_accuracy: 0.8502\n",
      "Epoch 22/25\n",
      "31/31 [==============================] - 4s 134ms/step - loss: 0.3180 - accuracy: 0.8746 - val_loss: 0.2487 - val_accuracy: 0.9001\n",
      "Epoch 23/25\n",
      "31/31 [==============================] - 4s 131ms/step - loss: 0.3578 - accuracy: 0.8736 - val_loss: 0.3444 - val_accuracy: 0.8675\n",
      "Epoch 24/25\n",
      "31/31 [==============================] - 4s 128ms/step - loss: 0.3160 - accuracy: 0.8777 - val_loss: 0.2463 - val_accuracy: 0.9083\n",
      "Epoch 25/25\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 0.3305 - accuracy: 0.8705 - val_loss: 0.2708 - val_accuracy: 0.9042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x188ab8537c0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=training_set,validation_data = training_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a28bda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('archive (2)\\CK+48\\surprise\\S011_001_00000015.png', target_size = (48,48))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e92271c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29195195, 0.0015313 , 0.14294416, 0.5718715 , 0.0012624 ,\n",
       "        0.7215737 , 0.9997586 ]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "95d72ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c674a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "10390e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=48x48 at 0x188AB87A1C0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('archive (2)\\CK+48\\surprise\\S011_001_00000015.png')\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9976ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img=cv2.imread('archive (2)\\CK+48\\sadness\\S014_002_00000015.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "048f0ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 3)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6055dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
